{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN7OpxsxOTFYaH8PWgcrVEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnema05/learning-LLMs/blob/main/small_scale_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a small scale LLM\n",
        "\n",
        "Implementing a small scale LLM with small dataset, character-level tokenizer\n",
        "\n",
        "References:\n",
        "\n",
        "[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=7)\n",
        "\n",
        "[Building LLMs from the Ground Up: A 3-hour Coding Workshop](https://www.youtube.com/watch?v=quh7z1q7-uc )\n"
      ],
      "metadata": {
        "id": "UwQTnko6ZNNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "# TensorFlow and PyTorch convert all data into tensors which are general purpose container\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qXJzV61EZi5Z",
        "outputId": "092e8670-539d-4a27-d81d-56a24b81c617"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2kpLpe4fjZVP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## STEP 1: Dataset!\n",
        "\n",
        "from datasets import load_dataset\n",
        "# small dataset for training small scale llms\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
        "print(dataset[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE18Bhlec8hO",
        "outputId": "50e60228-0e6f-493e-ee62-0327785065f5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvSiCg0flczy",
        "outputId": "6e1a4c06-107b-4d05-ac5e-98201e6ac926"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 2119719\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 21990\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## STEP 2: Get Text!\n",
        "\n",
        "# LLM needs sequence of text\n",
        "# extract text from dataset and combine into one long string\n",
        "  # you get the first 500 text samples.\n",
        "texts = [example[\"text\"] for example in dataset[\"train\"].select(range(500))]\n",
        "raw_text = \"\\n\".join(texts)\n",
        "\n",
        "print(\"Total characters:\", len(raw_text))\n",
        "print(\"Preview:\", raw_text[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBNhOLqJkE-K",
        "outputId": "2f498f53-c89b-4894-fd2f-c8eca2408c86"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 402933\n",
            "Preview: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
            "\n",
            "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## STEP 3: Create Tokenizer!\n",
        "\n",
        "# character level tokenizer\n",
        "  # this means each character in a text is a separate token\n",
        "  # has a fixed vocab: define it once and it maps char → int\n",
        "  # the tokenizer is a tool that converts text into tokens (in our case characters)\n",
        "  # then maps each character to a token id using a vocabulary (list of unique token)\n",
        "  # and uses the map to encode and decode text from token -> id and vice versa\n",
        "  # id's are so machine can process text with ints (and eventually vectors of ints)\n",
        "\n",
        "\n",
        "# get a list of unique characters that occur in this text\n",
        "# unique characters = vocabulary\n",
        "# the number of them is our vocabulary size\n",
        "\n",
        "chars = sorted(list(set(raw_text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Unique characters: {vocab_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcmYCB6PjqUd",
        "outputId": "7a3b7a06-1671-4476-ebf5-f1333b2c6871"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique characters: 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZTIexmyskEko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# continue tokenizer by making a map from token char -> int for our vocab!\n",
        "class CharTokenizer:\n",
        "    def __init__(self, text):\n",
        "        self.chars = sorted(list(set(text))) # gets vocabulary or unique char in text\n",
        "        self.vocab_size = len(self.chars)\n",
        "        # stoi is our dictionary mapping for each token char -> int\n",
        "        self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        # itos is dictionary mapping each int -> token char\n",
        "        self.itos = {i: ch for i, ch in enumerate(self.chars)}\n",
        "\n",
        "    def encode(self, s):\n",
        "        return [self.stoi[c] for c in s] # takes a string, returns a list of integers corresponding to each char in string\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return ''.join([self.itos[i] for i in ids]) # takes a list of integers, output a string"
      ],
      "metadata": {
        "id": "smp6xD2rkDWe"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = CharTokenizer(raw_text)\n",
        "\n",
        "encoded = tokenizer.encode(\"hellooo\")\n",
        "decoded = tokenizer.decode(encoded)\n",
        "\n",
        "print(\"Encoded:\", encoded)\n",
        "print(\"Decoded:\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXThzd1ZnBeY",
        "outputId": "cdadfc3e-768f-4c95-e150-e334150beff8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded: [49, 46, 53, 53, 56, 56, 56]\n",
            "Decoded: hellooo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## STEP 4: Turn tokenized text into training sequences!\n",
        "  # Create training data for the model by slicing your tokenized text into many short input→target pairs.\n",
        "  # LLMs are trained to predict next token\n",
        "  # so need training examples like:\n",
        "    # Input Sequence: \"The el\" -> Target Sequence: \"he elk\"\n",
        "    # Input Sequence: \"he elk\" -> Target Sequence: \"e elk \"\n",
        "\n",
        "# tensors are containers that work like a matrix  but extended to any number of dimensions (needed ML work is just matrix math)\n",
        "# encode text into list of token id's and store it in a tensor!\n",
        "data = torch.tensor(tokenizer.encode(raw_text), dtype=torch.long)\n",
        "print(data.shape, data.dtype) # 1D vector with 402933 tokens (vector of all our token ids)\n",
        "  # 402933 is number of characters in our raw text!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnejkSrvrktL",
        "outputId": "1c8aafd1-6740-4d8c-ef79-286fda599458"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([402933]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into training data and validation data\n",
        "# this is to get a sense of overfitting\n",
        "# overfitting occurs when the model memorizes/learns training data perfectly but but performs badly on new or unseen data\n",
        "# hiding part of data from model (validation_data) allows it to see how well it predicts on the unseen part\n",
        "  # training data is the only data model sees during training\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "LPvJF3pJvrab"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# block size = how many tokens the model sees at once\n",
        "  # this is the length of patterns the model can learn at a time\n",
        "  # we will never feed the entire text into transformer\n",
        "  # the chunks have a max size or maximum length\n",
        "  # maximum length is called block size!\n",
        "block_size = 64 # do 64 tokens\n",
        "\n",
        "# batch size is ow many independent training sequences (of size block size) are processed in parallel during one training step\n",
        "  # basically giving model 4 mini documents each 64 tokens long\n",
        "  # it reads each document independently but in parallel!\n",
        "  # this batching speeds up training!\n",
        "    # instead of updating the model 1 sequence at a time, we do 4 at once\n",
        "batch_size = 4"
      ],
      "metadata": {
        "id": "cp0raKezyZAp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to visualize how LLM will check tokens one at time\n",
        "# gets the input data up to block size and then the y is the target offset by 1\n",
        "  # so it includes the next token\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1] # y is next block size characters so its offset by 1\n",
        "\n",
        "# so for each block size (it checks the token before) and aims to predict the next target token\n",
        "  # for easier visualization in for loop make block size 8!\n",
        "small_block_size = 8\n",
        "for t in range(small_block_size):\n",
        "    context = x[:t+1] # get the all the context or all the characters right before t + 1\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFjeUmeP0UDE",
        "outputId": "d6720176-2024-4302-f8e4-91a57419f648"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([31]) the target: 55\n",
            "when input is tensor([31, 55]) the target: 46\n",
            "when input is tensor([31, 55, 46]) the target: 1\n",
            "when input is tensor([31, 55, 46,  1]) the target: 45\n",
            "when input is tensor([31, 55, 46,  1, 45]) the target: 42\n",
            "when input is tensor([31, 55, 46,  1, 45, 42]) the target: 66\n",
            "when input is tensor([31, 55, 46,  1, 45, 42, 66]) the target: 6\n",
            "when input is tensor([31, 55, 46,  1, 45, 42, 66,  6]) the target: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function that gets our batch of 4 block-sized seqeunces at a time\n",
        "# we pick 4 radnom starting positions i in the the text\n",
        "  # so then you extract x (input) and y (target) which are block sized seqeunces\n",
        "    # where target is one offset from input!\n",
        "    # random starting points for sequences gives you a new batch of examples everytime\n",
        "       # also avoids overfitting bc it prevents the model memorizing specific patterns at specific positions\n",
        "       #  model can’t rely on positional bias like “always start with ‘Once’”\n",
        "       # this is how it learns structure, not just surface patterns.\n",
        "def get_batch(split):\n",
        "    source = train_data if split == \"train\" else val_data # token id training data!\n",
        "    ix = torch.randint(len(source) - block_size, (batch_size,)) # picks batch size number of random positions\n",
        "    x = torch.stack([source[i:i+block_size] for i in ix]) # inputs\n",
        "      # extracts 4 slices of 64 tokens at random starting positions and stacks them in x\n",
        "    y = torch.stack([source[i+1:i+block_size+1] for i in ix]) # targets\n",
        "      # extract same 64 tokens but shifted 1 token forward\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch(\"train\")\n",
        "print(\"input batch dimensions:\", xb.shape) # (4, 64) (4 64 token sequences!)\n",
        "print('inputs:')\n",
        "print(xb)\n",
        "print(\"target batch dimensions:\", yb.shape)\n",
        "print('targets:')\n",
        "print(yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzXTTg6v07ke",
        "outputId": "393470da-6548-40b9-8cca-7d46e08abdcc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input batch dimensions: torch.Size([4, 64])\n",
            "inputs:\n",
            "tensor([[ 1,  3, 24, 46, 53, 53, 56,  1, 61, 49, 46, 59, 46,  6,  1, 25,  5, 54,\n",
            "          1, 61, 59, 62, 44, 52,  2,  1, 39, 56, 62, 53, 45,  1, 66, 56, 62,  1,\n",
            "         53, 50, 52, 46,  1, 61, 56,  1, 61, 42, 52, 46,  1, 42,  1, 59, 50, 45,\n",
            "         46, 16,  3,  0, 36, 49, 46,  1, 53, 50],\n",
            "        [ 1, 29, 42, 59, 66,  1, 64, 46, 55, 61,  1, 61, 56,  1, 61, 49, 46,  1,\n",
            "         57, 42, 59, 52,  6,  1, 52, 50, 44, 52, 46, 45,  1, 61, 49, 46,  1, 44,\n",
            "         50, 59, 44, 53, 46, 60,  1, 42, 59, 56, 62, 55, 45,  1, 47, 56, 59,  1,\n",
            "         42,  1, 64, 49, 50, 53, 46,  6,  1, 42],\n",
            "        [55, 48,  1, 50, 47,  1, 60, 49, 46,  1, 62, 60, 46, 45,  1, 49, 46, 59,\n",
            "          1, 50, 54, 42, 48, 50, 55, 42, 61, 50, 56, 55,  8,  0, 31, 55, 44, 46,\n",
            "          1, 62, 57, 56, 55,  1, 42,  1, 61, 50, 54, 46,  6,  1, 61, 49, 46, 59,\n",
            "         46,  1, 64, 42, 60,  1, 42,  1, 53, 50],\n",
            "        [64,  1, 50, 61,  1, 54, 42, 45, 46,  1, 49, 46, 59,  1, 47, 46, 46, 53,\n",
            "          1, 60, 54, 42, 59, 61, 46, 59,  8,  0,  0, 35, 49, 46,  1, 46, 63, 46,\n",
            "         55,  1, 64, 59, 56, 61, 46,  1, 42,  1, 59, 49, 66, 54, 46,  1, 42, 43,\n",
            "         56, 62, 61,  1, 49, 46, 59,  1, 64, 42]])\n",
            "target batch dimensions: torch.Size([4, 64])\n",
            "targets:\n",
            "tensor([[ 3, 24, 46, 53, 53, 56,  1, 61, 49, 46, 59, 46,  6,  1, 25,  5, 54,  1,\n",
            "         61, 59, 62, 44, 52,  2,  1, 39, 56, 62, 53, 45,  1, 66, 56, 62,  1, 53,\n",
            "         50, 52, 46,  1, 61, 56,  1, 61, 42, 52, 46,  1, 42,  1, 59, 50, 45, 46,\n",
            "         16,  3,  0, 36, 49, 46,  1, 53, 50, 61],\n",
            "        [29, 42, 59, 66,  1, 64, 46, 55, 61,  1, 61, 56,  1, 61, 49, 46,  1, 57,\n",
            "         42, 59, 52,  6,  1, 52, 50, 44, 52, 46, 45,  1, 61, 49, 46,  1, 44, 50,\n",
            "         59, 44, 53, 46, 60,  1, 42, 59, 56, 62, 55, 45,  1, 47, 56, 59,  1, 42,\n",
            "          1, 64, 49, 50, 53, 46,  6,  1, 42, 55],\n",
            "        [48,  1, 50, 47,  1, 60, 49, 46,  1, 62, 60, 46, 45,  1, 49, 46, 59,  1,\n",
            "         50, 54, 42, 48, 50, 55, 42, 61, 50, 56, 55,  8,  0, 31, 55, 44, 46,  1,\n",
            "         62, 57, 56, 55,  1, 42,  1, 61, 50, 54, 46,  6,  1, 61, 49, 46, 59, 46,\n",
            "          1, 64, 42, 60,  1, 42,  1, 53, 50, 61],\n",
            "        [ 1, 50, 61,  1, 54, 42, 45, 46,  1, 49, 46, 59,  1, 47, 46, 46, 53,  1,\n",
            "         60, 54, 42, 59, 61, 46, 59,  8,  0,  0, 35, 49, 46,  1, 46, 63, 46, 55,\n",
            "          1, 64, 59, 56, 61, 46,  1, 42,  1, 59, 49, 66, 54, 46,  1, 42, 43, 56,\n",
            "         62, 61,  1, 49, 46, 59,  1, 64, 42, 50]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## STEP 5: Build a transformer! (small only 2 layers!)\n",
        "  # like GPT is a decoder only transformer (which generates text by predicting the next token )\n",
        "\n",
        "  # training data gets passed through transformer\n",
        "  # transformer is a type of neural network (neural network is layers is  input layer (receives initial data), one or more hidden layers (perform computations), and an output layer)\n",
        "    # these connections have weights: Neurons within and between layers are connected\n",
        "    # network learns weights that help it make decisions (about what comes next between layers)\n",
        "    # weights are updated during training to reduce prediction error\n",
        "  # transformers handle sequences and sees tokens in a sequence at once, and uses attention\n",
        "  # transformers have two parts\n",
        "    # Self-attention: Every word can use other words in input as context\n",
        "    # Layered architecture: Transformers stack layers of attention + feedforward networks, each layer transforming the input\n"
      ],
      "metadata": {
        "id": "7w8XC0H45TSu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# variables needed for transformer\n",
        "batch_size = 4\n",
        "block_size = 64\n",
        "vocab_size = tokenizer.vocab_size  # number of unique characters\n",
        "# embedding is a way to turn an integer (like a token ID) into a vector\n",
        "embed_dim = 128  # each token becomes a vector of size 128\n",
        "\n",
        "# heads: One head = one way to look at relationships between tokens.\n",
        "# single head attention: learns how one token should pay attention to others in the same sequence\n",
        "num_layers = 2  # how many transformer blocks\n",
        "  # transformer block is a building unit that has Self-Attention, Feedforward MLP, LayerNorm + Residual Connections\n",
        "  # we want two layers of this two repeat this process twice\n",
        "  # because the deeper the network, the more learning of complex and abstract patterns"
      ],
      "metadata": {
        "id": "b8ZqGqJN5XbG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. first neural network component called Embedding layer!\n",
        "  # Why Embedding?: embedding is a way to turn an integer (like a token ID) into a vector\n",
        "  # the vector is the way neural network understand token id's\n",
        "  # thats how the transformer looks at every token (as a vector!)\n",
        "  # these vectors are learable: Embedding vectors are initialized randomly at the start\n",
        "  # During training: model predicts the next token, loss is calculated, then we do backpropagation to update the embedding values to reduce that error\n",
        "  # After training:  values in each token’s embedding vector determine how relationship of that token is to other tokens\n",
        "    # similar words should have similar vectors after training.\n",
        "\n",
        "  # AS THE VECTORS CHANGE: they are outputs from each of the layers!!\n",
        "    # they are changing as neurons do some math to chnage them!\n",
        "\n",
        "class EmbeddingLayer(nn.Module): ## nn.Module is a PyTorch base class (PyTorch way to define and run a neural network)\n",
        "  # build off of nn.Module to for anything that has layers, parameters, and it knows how to run forward computations\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, block_size):\n",
        "        super().__init__()\n",
        "        # creates a look up table where we map token IDs to vectors, each vector of length embed_dim and number of vectors is vocab size\n",
        "          # 64 by 128\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        # look up table of positions: transformers have no sense of order so it maps the 64 position indexes to a vector\n",
        "          # 64 by 128\n",
        "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
        "        # input = token_embedding + position_embedding = what token is + where token is in our block sequence\n",
        "\n",
        "    # forward() defines how data moves through the layers, this is the computation\n",
        "      # takes in the batch of tokens as an input and modifies it by emedding the tokens and passing it to next layer!\n",
        "    def forward(self, x): # x is a batch of token ID seqeunces (input into embedding layer!)\n",
        "\n",
        "        B, T = x.shape # B = batch size, T = time steps (number of tokens per seqeunce)\n",
        "        # token_embed is vector for our each of our tokens in x\n",
        "        token_embed = self.token_embedding(x)                   # (B, T, D) # D = emed_dim = size of vector\n",
        "        # tensor that has all positions in seqeunce [0, 1, .. T-1]\n",
        "        positions = torch.arange(T, device=x.device)           # (T,)\n",
        "        # one vector for each position T from position embedding look up table\n",
        "        pos_embed = self.position_embedding(positions)[None, :, :]  # (1, T, D)\n",
        "        # adds positional info to each vector token\n",
        "        return token_embed + pos_embed  # (B, T, D)\n",
        "          # this output is like a stack of 4 (B) matrices where each 64 rows (T) × 128 columns (D)\n",
        "            # so each token in batch is assoicated with its vector!\n",
        "          # output to that will be passed through later transfomer layers!\n",
        "\n",
        "# neuron in your transformer is something like: output = ReLU(w₁·x₁ + w₂·x₂ + ... + b)\n",
        "  # takes input numbers (like values in an embedding), Multiplies by weights, Adds a bias, passes through an activation, outputs a number"
      ],
      "metadata": {
        "id": "Tec-cWNmAccB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ByLzxR7Zf3G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention is a communication mechanism.\n",
        "# Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "# we can allow token vectors to talk to each other\n",
        "  # if we mask then tokens can only talk to tokens in the past!\n",
        "# masked self-attention does this! (allows each token to look at itself and every earlier token, decide how much to weigh each other token's contribution)\n",
        "# every single token emits two vectors, a query and a key\n",
        "# the query vector is what I\"m looking for and the key vector is what do I contain\n",
        "# do a dot product between key and query and that dot product now becomes attention weights\n",
        "  # attention weight = how much each token wants to focus on every other token in its sequence\n",
        "\n",
        "# self attention: each token wants to compute a new version of itself based on its own meaning and the context\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim): # embed dim: size of vector you are working with\n",
        "        super().__init__()\n",
        "        # each token vector is used to compute three seperate layers and get new vectors at each layer:\n",
        "          # linear layers have weights that control how input is turned into query, key and value\n",
        "          # these weughts are learned\n",
        "        self.key = nn.Linear(embed_dim, embed_dim, bias=False) # what do I contain\n",
        "        self.query = nn.Linear(embed_dim, embed_dim, bias=False) # what am I looking for\n",
        "          # each query vector is a way for this token to define what kinds of relationships it cares about\n",
        "            # or what tokens in seqeuence are relevant to me right now\n",
        "        self.value = nn.Linear(embed_dim, embed_dim, bias=False) # what information should I share with other tokens\n",
        "\n",
        "        # builds a lower triangular matrix mask\n",
        "          # so token at position t can only look at itself and tokens before it (not future tokens)\n",
        "            # to get context!\n",
        "            # bc LLM's are generating text sequentially, masked self-attention is used to prevent model from looking at future tokens that haven't been generated yet!\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        # dropout: randomly drops some neurons out and trains without them\n",
        "          # this trains a lot of subnetworks by doing this in each pass\n",
        "          # then at final testing it combines all the subnetworks as part of the neural network\n",
        "            # This forces the network to not rely too heavily on any single neurons or tokens\n",
        "        self.dropout = nn.Dropout(0.1) # drop out is\n",
        "\n",
        "  # forward() defines how data moves through the layers, this is the computation\n",
        "    def forward(self, x): # x is the input from the embedding layer of\n",
        "        B, T, C = x.shape # C = naming convention is channel = embed_dim = size of token vector\n",
        "        # gets key, qery, value vectors for each token in sequence\n",
        "        # one C-dim vector for each of T tokens in B batches\n",
        "        k = self.key(x)    # (B, T, C)\n",
        "        q = self.query(x)  # (B, T, C)\n",
        "        v = self.value(x)  # (B, T, C)\n",
        "\n",
        "        # each query then is dot-multiplied with all the key vectors in the sequence to compute\n",
        "          # How strongly does token A match token B?\n",
        "          # result is attention weights so each token now knows how much it wants to focus on every other token in its sequence.\n",
        "          # these are row scores (how well each token’s query matches every other token’s key in the sequenc)\n",
        "        attn_weights = q @ k.transpose(-2, -1) * (C ** -0.5)  # (B, T, T)\n",
        "        # applies the lower triangular mask (sets future tokens to -inf)\n",
        "          # ensures model predicts one token at a time using only the past\n",
        "        attn_weights = attn_weights.masked_fill(self.mask[:T, :T] == 0, float('-inf')) # -infinity\n",
        "        # turn raw scores into probabilities between 0-1 that sum to 1\n",
        "          # this is bc we want to compute weighted average of value vectors (later on)\n",
        "            # so weights need to be non-negative, sum to 1 so model can say \"I’ll take 70% of this token’s value, 20% of that one, and 10% of this other one\"\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        # dropout: 0's out some attention probabilities at random\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # compute weighted average of value vectors so multiple each value vector by how much it was atteneded to\n",
        "          # Each token gets its own query, which creates its own attention distribution:\n",
        "          # for each token you multiply attention weights for one token by value vectors of other tokens\n",
        "        # matrix multiplication does this for all my tokens\n",
        "        out = attn_weights @ v  # (B, T, C)\n",
        "        return out # output is made of new vector per token which has CONTEXT from other tokens\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "AsNajbKnAd0y"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer block\n",
        "  # run self attention:  builds context-aware vectors\n",
        "  # refines its own vector with feedforward network/Multi-Layer Perceptron\n",
        "  # adds in original token vectors with new computed vectors\n",
        "  # normalizes everything: scaling numerical data to a standard range\n",
        "# refining each token\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        # creates our self-attention block which will output our contextualized batch of token vectors\n",
        "        self.attn = SelfAttention(embed_dim)\n",
        "\n",
        "        # feedforward = Multi-Layer Perceptron\n",
        "        # feedfforward block is a small neural network that helps each token get refined\n",
        "          # adds complexity/nonlinearlity to each token seperately\n",
        "          # with nonlinearlity you can model complex, non-linear patterns\n",
        "        self.ffwd = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim), # exapnds vector space\n",
        "            nn.ReLU(), # adds nonlinearity (which allows for detection of complex things rather than just using linear functions)\n",
        "            nn.Linear(4 * embed_dim, embed_dim), # compresses the vector back after complexity is added\n",
        "        )\n",
        "        # defines layer normalization the vectors to ensure all token vectors are on same stable scale of numbers\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x): # x is our batch\n",
        "      # do the attention to return a new context-aware version of each token\n",
        "      # Residual connection: add original token to attention output to keep original token info in total output\n",
        "      # Normalizes result!\n",
        "        x = self.ln1(x + self.attn(x))   # residual connection + norm\n",
        "      # after each vector goes through the Multi-Layer Perceptron\n",
        "        # add another residual connection (original info about token added back into new computed output)\n",
        "        # normalzie result\n",
        "        x = self.ln2(x + self.ffwd(x))   # residual connection + norm\n",
        "        return x # (B, T, D) -> batch of refined token vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "elBAdTyMAe9-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full transformer with all of our layers\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # emedding layer: turns token id's into vectors with positional infomration for each token\n",
        "        self.embedding = EmbeddingLayer(vocab_size, embed_dim, block_size) # (B, T, D)\n",
        "        # num_layers of Transformer blocks (which applies self attention, feedforward and adds in residuals (old info))\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(embed_dim) for _ in range(num_layers)])\n",
        "        # layer norm which normalizes vector one last time\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        # final prediction layer which maps each token vector a vector of length vocab size\n",
        "        self.head = nn.Linear(embed_dim, vocab_size)  # maps final vector to vocab logits\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        tok = self.embedding(x)        # (B, T, D) # get embeddings\n",
        "        out = self.blocks(tok)         # (B, T, D) # pass through trasnformer blocks\n",
        "        out = self.ln_f(out)           # (B, T, D) # normalizes vectors\n",
        "        # gets logit scores!\n",
        "         # so for each token position in input seqeunce, model outputs corresponding vector of logit scores\n",
        "          # vector of logit scores is the size of vocab size\n",
        "          # each logit scores represent the raw prediction of how likely each possible next token is\n",
        "          # logit scores are NOT probabilities yet!\n",
        "        logits = self.head(out)        # (B, T, vocab_size)\n",
        "\n",
        "        # mode if not training (if no training targets are given just return logits)\n",
        "        if targets is None:\n",
        "            return logits\n",
        "\n",
        "        # else train! compute loss for training from the targets\n",
        "        B, T, C = logits.shape # logits is Batch × Time-stamp (token) predictions where each prediction has a vector of vocab size for logit scores\n",
        "        logits = logits.view(B*T, C) # reshapes logits and targets for cross entropy function\n",
        "          #B*T = produces total number of tokens in batch\n",
        "        targets = targets.view(B*T)\n",
        "        # computes loss by comparing predicted scores to the correct next-token IDs and gives 1 loss number\n",
        "          # loss number is small if model predicts correctly!\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    # generate to generate new tokens!\n",
        "    # gets initial input, runs the model forward, selects the next token, appends\n",
        "    # repeats until you've generated total max_new_tokens\n",
        "    @torch.no_grad() # tells pytorch to not track gradients during function\n",
        "      # gradients are used during training to update the model weights, calculates how wrong the model was and how to adjust weights for next time\n",
        "      # but since we are in generation (not training) there is no need to use gradients so it runs faster\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "      # idx is the token ids (B, T) B = batch size, T = length of input\n",
        "      # max new tokens is how many additional tokens to generate\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]  # crop sequence to to last block_size tokens\n",
        "            # run model to get logits!\n",
        "            logits = self(idx_cond) if isinstance(self.forward(idx_cond), torch.Tensor) else self(idx_cond)[0]\n",
        "            # logits shape before: (B, T, vocab_size) — one prediction per token\n",
        "            # After: (B, vocab_size) = prediction for the last token\n",
        "            logits = logits[:, -1, :]\n",
        "            # convert logits into probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # randomly pick one token from probablity distribution\n",
        "              # picking randomly instead of max allows for variation\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append to sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        # after generation additional new tokens, you return your entire generated sequence!\n",
        "        return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "P-Ty6T3g_K0u"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TinyTransformer()\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ooMEvKwJxjj",
        "outputId": "f71f16f9-54bc-4124-da44-25cda0d7eeb1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 390218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST MODEL WITHOUT TRAINING!\n",
        "model = model.to(device)\n",
        "# pick a starting character or sequence\n",
        "start_text = \"The elk runs through\"\n",
        "\n",
        "# encode it to token IDs\n",
        "start_ids = tokenizer.encode(start_text)\n",
        "\n",
        "# convert to tensor then move to device\n",
        "start_tensor = torch.tensor([start_ids], dtype=torch.long).to(device)\n",
        "\n",
        "# Generate 100 tokens after the start\n",
        "output_tensor = model.generate(start_tensor, max_new_tokens=100)\n",
        "\n",
        "# Convert tensor back to list of token IDs\n",
        "output_ids = output_tensor[0].tolist()\n",
        "\n",
        "# Decode back to string\n",
        "generated_text = tokenizer.decode(output_ids)\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print(generated_text)\n",
        "\n",
        "# The text is gibberish! This is because we haven't trained it!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II48fhClNoq6",
        "outputId": "1a546346-a901-44d3-a0f2-6853826216d5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "The elk runs throughq\"wo 1\"GkZUft '-sE$ku gUrqzOVdâ:L$g2bLœ“™kz\n",
            "s.q:JT™2KlœpZNDUHo!sLœr3Pz;?.t””LvvIVJ.pKufmf.cD3jwO;0zv\n"
          ]
        }
      ]
    }
  ]
}